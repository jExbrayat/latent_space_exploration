# Latent space exploration

Get good idea of the nature of **neural network latent spaces** with this repository.  

The MNIST digits dataset was autoencoded using a two dimensional latent space in order to be able to visualize it intuitively.  
Several autoencoder architectures were tried. The best and chosen one is a variational autoencoder featuring dense and dropout layers.  

All tested architectures are logged in ``docs/models_characteristics.md``.  

<img src="assets/vae_latent_space_evolution.gif" width="700" height="700" />
